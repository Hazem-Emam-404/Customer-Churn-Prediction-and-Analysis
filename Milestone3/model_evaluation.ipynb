{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42efe38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            classification_report, confusion_matrix, roc_curve, \n",
    "                            roc_auc_score, precision_recall_curve, average_precision_score,\n",
    "                            matthews_corrcoef, balanced_accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "603b0af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved models and data...\n"
     ]
    }
   ],
   "source": [
    "# Load saved models and data\n",
    "print(\"Loading saved models and data...\")\n",
    "import pickle\n",
    "with open('churn_models_and_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "trained_models = data['models']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "feature_names = data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f950721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('evaluation_plots'):\n",
    "    os.makedirs('evaluation_plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bac8cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL EVALUATION ===\n"
     ]
    }
   ],
   "source": [
    "# MODEL EVALUATION\n",
    "print(\"\\n=== MODEL EVALUATION ===\")\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Balanced Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'AUC': [],\n",
    "    'MCC': []  # Matthews Correlation Coefficient - good for imbalanced data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc2f409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save confusion matrix with detailed analysis\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate confusion matrix statistics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    total = tn + fp + fn + tp\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot standard confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title(f'{model_name} Confusion Matrix')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('Actual')\n",
    "    \n",
    "    # Create a matrix for the right side with percentages and explanations\n",
    "    cm_stats = np.array([\n",
    "        [f\"TN: {tn}\\n({tn/total:.1%})\\nTrue Negatives\\n(Correctly predicted\\nnon-churned)\", \n",
    "         f\"FP: {fp}\\n({fp/total:.1%})\\nFalse Positives\\n(Incorrectly predicted\\nas churned)\"],\n",
    "        [f\"FN: {fn}\\n({fn/total:.1%})\\nFalse Negatives\\n(Missed actual\\nchurned customers)\", \n",
    "         f\"TP: {tp}\\n({tp/total:.1%})\\nTrue Positives\\n(Correctly predicted\\nchurned)\"]\n",
    "    ])\n",
    "    \n",
    "    # Plot the statistics explanation\n",
    "    ax2.axis('off')\n",
    "    ax2.table(cellText=cm_stats, loc='center', cellLoc='center', \n",
    "              colLabels=['Predicted NO', 'Predicted YES'],\n",
    "              rowLabels=['Actual NO', 'Actual YES'])\n",
    "    ax2.set_title('Confusion Matrix Explanation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'evaluation_plots/{model_name.replace(\" \", \"_\")}_confusion_matrix.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fc45b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save ROC curve\n",
    "def plot_roc_curve(y_true, y_proba, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'evaluation_plots/{model_name.replace(\" \", \"_\")}_roc_curve.png')\n",
    "    plt.close()\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a23d141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save Precision-Recall curve\n",
    "def plot_precision_recall_curve(y_true, y_proba, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    avg_precision = average_precision_score(y_true, y_proba)\n",
    "    plt.plot(recall, precision, label=f'AP = {avg_precision:.3f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'evaluation_plots/{model_name.replace(\" \", \"_\")}_pr_curve.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ece680b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Logistic Regression ---\n",
      "\n",
      "Detailed Metrics for Logistic Regression:\n",
      "Accuracy: 0.9691\n",
      "Balanced Accuracy: 0.9586\n",
      "Precision: 0.8268\n",
      "Recall: 0.9447\n",
      "F1 Score: 0.8818\n",
      "AUC: 0.9957\n",
      "Matthews Correlation Coefficient: 0.8667\n",
      "\n",
      "Confusion Matrix Analysis:\n",
      "True Negatives: 19751\n",
      "False Positives: 558\n",
      "False Negatives: 156\n",
      "True Positives: 2663\n",
      "Specificity (True Negative Rate): 0.9725\n",
      "False Positive Rate: 0.0275\n",
      "False Negative Rate: 0.0553\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     20309\n",
      "           1       0.83      0.94      0.88      2819\n",
      "\n",
      "    accuracy                           0.97     23128\n",
      "   macro avg       0.91      0.96      0.93     23128\n",
      "weighted avg       0.97      0.97      0.97     23128\n",
      "\n",
      "\n",
      "--- Evaluating Random Forest ---\n",
      "\n",
      "Detailed Metrics for Random Forest:\n",
      "Accuracy: 0.9655\n",
      "Balanced Accuracy: 0.8747\n",
      "Precision: 0.9525\n",
      "Recall: 0.7545\n",
      "F1 Score: 0.8420\n",
      "AUC: 0.9942\n",
      "Matthews Correlation Coefficient: 0.8300\n",
      "\n",
      "Confusion Matrix Analysis:\n",
      "True Negatives: 20203\n",
      "False Positives: 106\n",
      "False Negatives: 692\n",
      "True Positives: 2127\n",
      "Specificity (True Negative Rate): 0.9948\n",
      "False Positive Rate: 0.0052\n",
      "False Negative Rate: 0.2455\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     20309\n",
      "           1       0.95      0.75      0.84      2819\n",
      "\n",
      "    accuracy                           0.97     23128\n",
      "   macro avg       0.96      0.87      0.91     23128\n",
      "weighted avg       0.97      0.97      0.96     23128\n",
      "\n",
      "\n",
      "--- Evaluating Gradient Boosting ---\n",
      "\n",
      "Detailed Metrics for Gradient Boosting:\n",
      "Accuracy: 0.9866\n",
      "Balanced Accuracy: 0.9870\n",
      "Precision: 0.9098\n",
      "Recall: 0.9876\n",
      "F1 Score: 0.9471\n",
      "AUC: 0.9993\n",
      "Matthews Correlation Coefficient: 0.9404\n",
      "\n",
      "Confusion Matrix Analysis:\n",
      "True Negatives: 20033\n",
      "False Positives: 276\n",
      "False Negatives: 35\n",
      "True Positives: 2784\n",
      "Specificity (True Negative Rate): 0.9864\n",
      "False Positive Rate: 0.0136\n",
      "False Negative Rate: 0.0124\n",
      "\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     20309\n",
      "           1       0.91      0.99      0.95      2819\n",
      "\n",
      "    accuracy                           0.99     23128\n",
      "   macro avg       0.95      0.99      0.97     23128\n",
      "weighted avg       0.99      0.99      0.99     23128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in trained_models.items():\n",
    "    print(f\"\\n--- Evaluating {name} ---\")\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "  \n",
    "    \n",
    "    y_test = pd.Series(y_test).astype(int)\n",
    "    y_pred = pd.Series(y_pred).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results['Model'].append(name)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "    results['Balanced Accuracy'].append(balanced_acc)\n",
    "    results['Precision'].append(precision)\n",
    "    results['Recall'].append(recall)\n",
    "    results['F1 Score'].append(f1)\n",
    "    results['AUC'].append(auc)\n",
    "    results['MCC'].append(mcc)\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print(f\"\\nDetailed Metrics for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "    \n",
    "    # Calculate confusion matrix metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    false_positive_rate = fp / (fp + tn)\n",
    "    false_negative_rate = fn / (fn + tp)\n",
    "    \n",
    "    print(\"\\nConfusion Matrix Analysis:\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "    print(f\"False Negative Rate: {false_negative_rate:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred, name)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plot_roc_curve(y_test, y_proba, name)\n",
    "    \n",
    "    # Plot precision-recall curve\n",
    "    plot_precision_recall_curve(y_test, y_proba, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66276717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed and saved to 'model_evaluation_results.csv'\n",
      "Evaluation plots have been saved to the 'evaluation_plots' directory\n",
      "\n",
      "Best models by metric:\n",
      "Best model by Accuracy: Gradient Boosting (0.9866)\n",
      "Best model by Precision: Random Forest (0.9525)\n",
      "Best model by Recall: Gradient Boosting (0.9876)\n",
      "Best model by F1 Score: Gradient Boosting (0.9471)\n",
      "Best model by AUC: Gradient Boosting (0.9993)\n",
      "Best model by MCC: Gradient Boosting (0.9404)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in trained_models.items():\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.savefig('evaluation_plots/roc_curve_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Create comparison of metrics\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.set_index('Model')\n",
    "\n",
    "# Save detailed metrics to CSV\n",
    "results_df.to_csv('../data/model_evaluation_results.csv')\n",
    "\n",
    "# Create radar chart for model comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'MCC']\n",
    "models = results_df.index\n",
    "\n",
    "# Set up the radar chart\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "# Set the angles for each metric\n",
    "angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Close the loop\n",
    "\n",
    "# Plot each model\n",
    "for i, model in enumerate(models):\n",
    "    values = results_df.loc[model, metrics].values.tolist()\n",
    "    values += values[:1]  # Close the loop\n",
    "    \n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=model)\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_title('Model Performance Comparison (Radar Chart)', size=15)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_plots/radar_chart_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot bar chart comparison of metrics\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    results_df[metric].plot(kind='bar')\n",
    "    plt.title(f'Model Comparison - {metric}')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_plots/metrics_comparison_detailed.png')\n",
    "plt.close()\n",
    "\n",
    "# Create another bar chart with all metrics together\n",
    "plt.figure(figsize=(14, 8))\n",
    "results_df[metrics].plot(kind='bar')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_plots/metrics_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nEvaluation completed and saved to 'model_evaluation_results.csv'\")\n",
    "print(\"Evaluation plots have been saved to the 'evaluation_plots' directory\")\n",
    "\n",
    "# Display the best model based on different metrics\n",
    "print(\"\\nBest models by metric:\")\n",
    "for metric in metrics:\n",
    "    best_model = results_df[metric].idxmax()\n",
    "    best_score = results_df.loc[best_model, metric]\n",
    "    print(f\"Best model by {metric}: {best_model} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473e53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
